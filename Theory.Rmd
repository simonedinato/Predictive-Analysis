---
title: "Riassunto Analisi Predittiva"
output: html_notebook
author: Simone Dinato
date: "Data di Creazione: 2023-10-20"
version: "Versione: 0.6"
editor_options: 
  markdown: 
    wrap: sentence
---

# Introduzione

Questo documento fornisce un'ampia panoramica sul corso di Analisi Predittiva di Ca' Foscari (CT0429) dell'anno 2023/2024.
Nel corso del documento, esploreremo vari argomenti riguardanti l'Analisi Predittiva, compresi Anova, Residui, Formule, Correlazione tra variabili, Predict, l'utilizzo di più modelli, e come selezionare le variabili all'interno di un modello.

L'obiettivo di questo documento è guidarti attraverso i concetti chiave e le pratiche nell'Analisi Predittiva, con un focus sull'utilizzo del linguaggio di programmazione R per applicare queste tecniche.
Spero che questo documento ti aiuti a comprendere meglio questa materia di studio al fine di superare l'esame.

## Summary

-   [Analisi dei residui]
-   [Decomposition sum of squares]
-   [Optimal Prediction]
-   [Empirical Covariance and Correlation]
    -   [Empirical Covariance]
    -   [Empirical Correlation]
-   [Multiple Linear Regression]
-   [Matrix approach to Regression]
-   [Geometrical Interpretation]
-   [Distribuzione F & Anova Table](#distribuzione-f-anova-table)
    -   [Ipotesi]
-   [Nested Models]
-   [Variable Selection in R]
-   [Categorical Predictors and Interactions]
    -   [Factors with More Than Two Levels]
-   [Model Checking]
-   [Transformations]
-   [Multicollinearity]
-   [Influential points]

## Analisi dei residui

Serve a verificare se il modello soddisfa le assunzioni della regressione e per identificare eventuali pattern o problemi nei dati.

Esempio 1:

```{r}
# Generiamo dati casuali con residui normalmente distribuiti
set.seed(123)
x <- 1:100
y <- 2 * x + rnorm(100)

# Adattiamo un modello di regressione
model <- lm(y ~ x)

# Effettuiamo l'analisi dei residui
residuals <- residuals(model)

# Creiamo un grafico dei residui
plot(residuals, main = "Distribuzione Normale dei Residui", xlab = "Residui")
abline(h = 0, col = "red")
```

In questo caso, i residui seguono una distribuzione normale, il che è un risultato ideale per un modello di regressione lineare.
Quando diciamo che i residui hanno una distribuzione normale, significa che i residui seguono una distribuzione a forma di campana, con una media di zero e una varianza costante.
Questa è un'importante assunzione nei modelli di regressione lineare, in quanto indica che gli errori casuali nel modello sono distribuiti in modo simmetrico intorno a zero e non mostrano alcun tipo di tendenza sistemica.
Se questa assunzione è soddisfatta, i test di significatività dei coefficienti del modello e le stime di intervallo di confidenza saranno affidabili.

Esempio 2:

```{r}
# Generiamo dati casuali con residui che seguono una distribuzione a U
set.seed(456)
x <- 1:100
y <- 2 * x^2 + rnorm(100)
y[50:60] <- y[50:60] + 10  # Introduciamo un effetto a U nei dati

# Adattiamo un modello di regressione
model <- lm(y ~ x)

# Effettuiamo l'analisi dei residui
residuals <- residuals(model)

# Creiamo un grafico di dispersione dei residui rispetto ai valori previsti
plot(predict(model), residuals, main = "Distribuzione a U dei Residui", xlab = "Valori Previsti", ylab = "Residui")
abline(h = 0, col = "red")
```

In questo caso, i residui mostrano un effetto a U, indicando una violazione dell'assunzione di omoschedasticità(i residui non mostrano un aumento o una diminuzione sistematica nella dispersione al variare dei valori delle variabili indipendenti).

Esempio 3:

```{r}
# Generiamo dati casuali con outlier nei residui
set.seed(789)
x <- 1:100
y <- 2 * x + rnorm(100)
y[c(20, 85)] <- y[c(20, 85)] + 20  # Aggiungiamo outlier nei dati

# Adattiamo un modello di regressione
model <- lm(y ~ x)

# Effettuiamo l'analisi dei residui
residuals <- residuals(model)

# Creiamo un grafico dei residui
plot(residuals, main = "Presenza di Outlier nei Residui", xlab = "Residui")
abline(h = 0, col = "red")
```

In questo caso, i residui mostrano la presenza di outlier evidenti, che possono influenzare in modo significativo la stima dei coefficienti del modello.
Gli outlier possono comportare problemi nei modelli statistici, specialmente nei modelli di regressione, perché possono influenzare notevolmente i risultati.
Ad esempio, possono influenzare la stima dei coefficienti del modello e rendere il modello meno affidabile.

## Decomposition sum of squares

La "decomposition of sum of squares" (scomposizione della somma dei quadrati) è un concetto fondamentale nell'analisi della varianza (ANOVA) e nella regressione statistica.
Questa tecnica aiuta a scomporre la varianza totale osservata in un insieme di dati in diverse componenti, consentendo di comprendere quanto della varianza può essere attribuito a vari fattori o errori residui.
La formula chiave in questo contesto è:

$$ Varianza Totale = Varianza Spiegata + Varianza Residua $$

Dove:

-   Varianza Totale è la varianza complessiva dei dati, cioè quanto i dati variano in generale.

-   Varianza Spiegata rappresenta la varianza dovuta al modello o ai fattori esaminati (spiegati dalla variabile indipendente nel contesto della regressione).

-   Varianza Residua è la varianza non spiegata dal modello o dai fattori ed è associata all'errore residuo, ovvero la differenza tra i valori osservati e quelli previsti dal modello.

Nel contesto della regressione, puoi rappresentare la decomposizione della somma dei quadrati come segue:

$$ SST = SSR + SSE $$

-   SST (Sum of Squares Total) rappresenta la somma dei quadrati totale ed è la varianza dei dati osservati rispetto alla loro media.

-   SSR (Sum of Squares Regression) rappresenta la varianza spiegata dal modello o dalla variabile indipendente.

-   SSE (Sum of Squares Error) rappresenta la varianza residua, ossia la varianza non spiegata dal modello.

Per valutare l'efficienza del tuo modello di regressione, dovresti guardare la proporzione di questa varianza spiegata dal tuo modello (SSR).
In generale, vuoi massimizzare la proporzione spiegata e minimizzare la proporzione non spiegata (SSE).
Pertanto, punti a minimizzare SSE.

```{r}
data <- data.frame(X = c(1, 2, 3, 4, 5), Y = c(3, 5, 6, 8, 10))
mean_Y <- mean(data$Y)
SST <- sum((data$Y - mean_Y)^2)

# Adatta il modello di regressione lineare
model <- lm(Y ~ X, data = data)

# Calcola la SSR
SSR <- sum((predict(model) - mean_Y)^2)

# Calcola la SSE
SSE <- sum(model$residuals^2)

R_squared <- SSR / SST

# Equivale a fare summary(model)
R_squared
```

## Optimal Prediction

L'argomento dell'"Optimal Prediction" (predizione ottimale) riguarda la determinazione di un modello predittivo che sia il migliore possibile in termini di accuratezza nel prevedere gli eventi futuri.

L'obiettivo principale dell'optimal prediction è trovare il modello che massimizza la precisione delle previsioni, minimizzando l'errore di previsione.
Ci sono vari metodi e tecniche per ottenere la predizione ottimale, a seconda del contesto e dei dati disponibili.

```{r}
# Caricamento del dataset "cars"
data(cars)

# Visualizzazione delle prime righe del dataset
head(cars)

# Dividiamo il dataset in set di addestramento e set di test
set.seed(123)  # Impostiamo un seed per la riproducibilità
sample_indices <- sample(nrow(cars), nrow(cars) * 0.7)  # 70% dati di addestramento
train_data <- cars[sample_indices, ]
test_data <- cars[-sample_indices, ]

# Adattamento di un modello di regressione lineare
model <- lm(dist ~ speed, data = train_data)

# Predizioni
predictions <- predict(model, newdata = test_data)

# Valutazione delle prestazioni
model_summary <- summary(model)
r_squared <- model_summary$r.squared

cat("R-squared:", r_squared, "\n")

# Grafico dei risultati
library(ggplot2)
ggplot(data = test_data, aes(x = speed, y = dist)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  ggtitle("Predizione della Distanza di Arresto")

```

## Empirical Covariance and Correlation

L'empirical covariance e la correlazione empirica sono misure statistiche utilizzate per quantificare la relazione tra due variabili in un insieme di dati osservati.
Queste misure sono strettamente legate e sono spesso utilizzate per esaminare la relazione lineare tra due variabili.

#### Empirical Covariance

La covarianza empirica è una misura della tendenza di due variabili a variare insieme.
Indica se le due variabili crescono o diminuiscono simultaneamente (covarianza positiva) o se una aumenta mentre l'altra diminuisce (covarianza negativa).
La formula per calcolare la covarianza empirica tra due variabili X e Y in un set di dati è data da:

$$  Cov(X,Y) = \frac{1}{n-1} \sum_{i = 1}^n {(X_i - \overline{X})(Y_i - \overline{Y})} $$

#### Empirical Correlation

La correlazione empirica è una versione standardizzata della covarianza empirica e misura la forza e la direzione di una relazione lineare tra due variabili.
La correlazione empirica è sempre compresa tra -1 e 1.
La formula per calcolare la correlazione empirica (il coefficiente di correlazione di Pearson) tra due variabili X e Y è data da:

$$  Cor(X,Y) = \frac{Cov(X,Y)}{S_X \cdot S_Y} $$

Le misure di covarianza empirica e correlazione empirica sono utilizzate per esaminare la relazione tra variabili in un set di dati e sono particolarmente utili nell'analisi statistica e nell'apprendimento automatico per valutare le associazioni tra le variabili prima di costruire modelli predittivi.
La correlazione empirica è più comunemente utilizzata perché fornisce una misura standardizzata della relazione tra variabili ed è meno influenzata dall'unità di misura.

Il coefficiente di Pearson è utile per vedere se due variabili hanno una correlazione lineare o meno.
Questo perché non tutte le variabili correlate hanno una relazione lineare.

Esempio:

```{r}
# Esempio dati casuali
set.seed(123)
x <- rnorm(100)  # Variabile x
y <- 2 * x + rnorm(100)  # Variabile y (correlata a x)

# Calcola la correlazione di Pearson
correlation <- cor(x, y)

# Stampa il valore di correlazione
cat("Correlazione di Pearson tra x e y:", correlation, "\n")
```

In questo esempio, stiamo generando dati casuali per le variabili x e y.
La variabile y è costruita come una trasformazione lineare di x con un termine di errore aggiunto.
Poi, utilizziamo la funzione cor() per calcolare la correlazione di Pearson tra x e y.

Il valore di correlazione verrà stampato a schermo.
Un valore vicino a 1 indica una correlazione positiva forte, un valore vicino a -1 indica una correlazione negativa forte, mentre un valore vicino a 0 indica una scarsa correlazione tra le due variabili.

## Multiple Linear Regression

La "Multiple Linear Regression" (Regressione Lineare Multipla) è una tecnica di modellazione statistica utilizzata per analizzare la relazione tra una variabile dipendente (o target) e due o più variabili indipendenti (o predittive).
Questa tecnica estende la semplice regressione lineare, che coinvolge solo una variabile indipendente, a un contesto in cui più variabili indipendenti sono coinvolte nel modello.
La regressione lineare multipla è ampiamente utilizzata nell'analisi statistica e nell'apprendimento automatico per fare previsioni o comprendere le relazioni complesse tra variabili.

$$ Y = \beta_0 + \beta_1X_1 + ... + \beta_nX_n + \epsilon $$ L'obiettivo principale è stimare i coefficienti β in modo che il modello si adatti meglio ai dati osservati.
Questo viene fatto utilizzando metodi di stima, come il metodo dei minimi quadrati, che cerca di minimizzare la somma dei quadrati degli errori residui.
Il modello di regressione viene valutato utilizzando metriche di valutazione delle prestazioni come l'errore quadratico medio (RMSE), il coefficiente di determinazione (R-squared) e altri.
È importante eseguire test di significatività statistica per i coefficienti delle variabili indipendenti per determinare se esse contribuiscono significativamente al modello.

La regressione lineare multipla è basata su alcune assunzioni, tra cui l'indipendenza degli errori, l'omoschedasticità (varianza costante degli errori), la linearità della relazione e la normalità degli errori.

```{r}
# Carica il dataset mtcars
data(mtcars)

# Visualizza le prime righe del dataset
head(mtcars)

# Adattamento del modello di regressione lineare multipla
model <- lm(mpg ~ wt + hp + qsec, data = mtcars)

# Visualizza un riepilogo del modello
summary(model)

# Effettua previsioni con il modello
predictions <- predict(model, newdata = mtcars)

# Valuta le prestazioni del modello
SSR <- sum((mtcars$mpg - predictions)^2)  # Somma dei quadrati residui
SST <- sum((mtcars$mpg - mean(mtcars$mpg))^2)  # Somma totale dei quadrati

R_squared <- 1 - (SSR / SST)

cat("R-squared (manual calculation):", R_squared, "\n")
```

In sintesi, il modello di regressione lineare multipla suggerisce che il peso del veicolo influenza il consumo di carburante.
La potenza del motore e il tempo di accelerazione non sono significative per la spiegazione di questo modello.
Nonostante ciò il modello spiega l'83% dei della variazione nei consumi di carburante.

## Matrix approach to Regression

L'approccio matriciale alla regressione è una forma alternativa di rappresentazione e risoluzione dei modelli di regressione, inclusa la regressione lineare.
Questo approccio utilizza notazioni matematiche e matrici per semplificare i calcoli e ottenere soluzioni più efficienti in problemi di regressione lineare.

Ci permette di passare da: $$ Y = \beta_0 + \beta_1X_1 + ... + \beta_nX_n + \epsilon $$ a : $$ Y = \beta X + \epsilon   $$

Per stimare i coefficienti β del modello, si utilizzano metodi come il metodo dei minimi quadrati.
L'obiettivo è trovare i valori di β che minimizzano la somma dei quadrati degli errori (ε).

a soluzione matriciale per stimare i coefficienti β é: $$ \beta = ((X^T X)^{-1} X^T Y) $$

L'approccio matriciale semplifica la rappresentazione e la risoluzione dei modelli di regressione, specialmente quando si lavora con più variabili indipendenti.
Inoltre, è utile per comprendere come eseguire calcoli di regressione in modo più efficiente utilizzando matrici e algebra lineare, specialmente in contesti di apprendimento automatico in cui le dimensioni dei dati possono essere elevate.

```{r}
# Carica il dataset "swiss" (un dataset di dati demografici svizzeri)
data(swiss)

# Visualizza le prime righe del dataset
head(swiss)

# Crea la matrice delle variabili indipendenti
X <- as.matrix(swiss[, c("Examination", "Education")])

# Aggiungi una colonna di 1 per l'intercetta
X <- cbind(1, X)

# Crea il vettore delle variabili dipendenti
Y <- swiss$Fertility

# Calcola i coefficienti del modello utilizzando l'approccio matriciale
beta <- solve(t(X) %*% X) %*% t(X) %*% Y

# Visualizza i coefficienti del modello
cat("Coefficienti del modello:")
print(beta)

# Effettua previsioni con il modello matriciale
predictions <- X %*% beta

# Valuta le prestazioni del modello
model <- lm(Y ~ 0 + predictions)  # 0 indica di non calcolare l'intercetta
summary(model)$r.squared  
# Restituisce il valore di R-squared
```

-   Esame (Examination): Un aumento nei punteggi di esame è associato a una diminuzione della fertilità nelle regioni svizzere. Questo suggerisce che un migliore stato di salute generale, misurato tramite l'esame, è correlato a una fertilità più bassa.
-   Educazione (Education): Un aumento nel livello di educazione è correlato a una riduzione della fertilità. Le regioni con un livello di istruzione più elevato tendono ad avere una fertilità più bassa.

## Geometrical Interpretation

L'interpretazione geometrica della regressione lineare è un approccio concettuale che utilizza uno spazio tridimensionale (o superiore) per rappresentare visivamente il modello di regressione.
In questo spazio, ogni punto rappresenta un'osservazione nel dataset, e un piano (o iperpiano) rappresenta il modello di regressione.
L'obiettivo è trovare il piano (o iperpiano) che minimizza la somma dei quadrati delle distanze verticali tra i punti dati e il piano (o iperpiano).
Questo fornisce una visualizzazione intuitiva di come i coefficienti del modello vengono stimati per ottenere la migliore "ajustement" ai dati, minimizzando gli errori residui.
L'interpretazione geometrica aiuta a comprendere i principi fondamentali della regressione lineare e può essere applicata a problemi più complessi con più variabili indipendenti.

```{r}
# Genera dati casuali
set.seed(123)
n <- 50
X1 <- rnorm(n)
X2 <- rnorm(n)
Y <- 2 * X1 + 3 * X2 + rnorm(n)

# Crea un dataframe con le variabili
data <- data.frame(X1, X2, Y)

# Adatta il modello di regressione lineare
model <- lm(Y ~ X1 + X2, data = data)

# Estrai i coefficienti stimati
coefficients <- coef(model)

# Visualizza i coefficienti
print(coefficients)

# Grafico dei dati e del piano di regressione
library(rgl)

# Creazione di una griglia di punti
x1_range <- seq(min(X1), max(X1), length = 20)
x2_range <- seq(min(X2), max(X2), length = 20)
grid <- expand.grid(X1 = x1_range, X2 = x2_range)

# Calcolo delle previsioni del modello sulla griglia
grid$Y_pred <- predict(model, newdata = grid)

# Creazione del plot 3D
plot3d(data$X1, data$X2, data$Y, type = "s", size = 0.5, col = "blue", xlab = "X1", ylab = "X2", zlab = "Y")
surface3d(x = x1_range, y = x2_range, z = matrix(grid$Y_pred, nrow = length(x1_range)), color = "red", alpha = 0.7)
```

L'interpretazione geometrica ci consente di vedere come il piano di regressione si adatta ai dati nello spazio tridimensionale e come i coefficienti stimati influenzano la posizione e l'inclinazione del piano rispetto ai dati osservati.
Questo fornisce una visualizzazione intuitiva della relazione tra le variabili indipendenti e dipendenti nel contesto della regressione lineare.

## Distribuzione F & Anova Table {#distribuzione-f-anova-table}

La "distribuzione F" è una distribuzione di probabilità usata per confrontare varianze o dispersioni tra gruppi o modelli di regressione.
Viene ampiamente utilizzata nell'analisi statistica, in particolare nelle analisi della varianza (ANOVA) e nei test di significatività dei modelli di regressione.

La "tabella ANOVA" è uno strumento che calcola le varianze spiegate e non spiegate nei dati, consentendo di testare se ci sono differenze significative tra gruppi o fattori.
La tabella ANOVA mostra la varianza tra gruppi e la varianza all'interno dei gruppi e calcola il "rapporto F", che viene utilizzato per valutare la significatività delle differenze.

In sintesi, la distribuzione F e la tabella ANOVA sono strumenti fondamentali nell'analisi statistica per testare e valutare le differenze tra gruppi o modelli di regressione.

Esempio 1:

```{r}
# Creiamo un dataset fittizio
set.seed(123)
data <- data.frame(
  Gruppo = rep(c("A", "B", "C"), each = 20),
  Punteggio = rnorm(60, mean = c(70, 75, 80), sd = 5)
)

# Eseguiamo l'ANOVA
anova_result <- aov(Punteggio ~ Gruppo, data = data)

# Visualizziamo la tabella ANOVA
summary(anova_result)

# Calcoliamo il rapporto F
f_statistic <- summary(anova_result)[[1]][["F value"]][1]

# Calcoliamo il p-value associato al rapporto F
p_value <- pf(f_statistic, df1 = 2, df2 = 57, lower.tail = FALSE)

cat("Rapporto F:", f_statistic, "\n")
cat("Valore p:", p_value, "\n")
```

In questo esempio, eseguiamo un'ANOVA a un fattore per valutare le differenze nei punteggi tra i gruppi A, B e C.
Il rapporto F e il valore p ci permettono di determinare se le differenze tra i gruppi sono statisticamente significative.

I risultati dell'ANOVA suggeriscono che non ci sono differenze statisticamente significative tra i gruppi.
La varianza tra i gruppi è molto piccola rispetto alla varianza all'interno dei gruppi, e il test F non è significativo.
Questo può indicare che i gruppi sono simili tra loro per quanto riguarda la variabile in studio.

Esempio 2:

```{r}
# Creiamo un dataset fittizio
set.seed(123)
data <- data.frame(
  Genere = rep(c("Maschio", "Femmina"), each = 50),
  Trattamento = rep(c("A", "B"), times = 50),
  Punteggio = rnorm(100, mean = c(75, 80), sd = 5)
)

# Eseguiamo l'ANOVA a due fattori
anova_result <- aov(Punteggio ~ Genere * Trattamento, data = data)

# Visualizziamo la tabella ANOVA
summary(anova_result)
```

In questo esempio, eseguiamo un'ANOVA a due fattori per esaminare le differenze nei punteggi in base al genere e al trattamento.
La tabella ANOVA ci mostra se ci sono interazioni significative tra questi due fattori.

Dalla tabella dell'ANOVA fornita, possiamo trarre le seguenti conclusioni:

-   Il fattore "Trattamento" ha un forte effetto sulla variabile dipendente, con un valore di F elevato e un p-value molto basso, indicando che le differenze tra i trattamenti sono statisticamente significative (\*\*\*).
-   Il fattore "Genere" non ha un effetto significativo sulla variabile dipendente, con un valore di F basso e un p-value elevato (0.546).
-   L'interazione tra "Genere" e "Trattamento" non ha un effetto significativo sulla variabile dipendente, con un valore di F e un p-value non significativi (0.121 e 0.729, rispettivamente).

In sintesi, i risultati dell'ANOVA suggeriscono che il "Trattamento" è il principale driver delle differenze osservate nella variabile dipendente, mentre il "Genere" e l'interazione tra "Genere" e "Trattamento" non sembrano avere un effetto significativo.

-   Un valore F maggiore di 1 suggerisce che i parametri o i fattori sono significativi, poiché la varianza spiegata è maggiore della varianza non spiegata.
-   Un valore F vicino a 1 indica che il modello non spiega in modo significativo la variabilità nei dati.
-   Il valore p associato all'F-value fornisce la probabilità che i risultati osservati siano dovuti al caso. Un valore p basso (di solito inferiore a 0.05) indica una significatività elevata, mentre un valore p alto suggerisce una mancanza di significatività.

La valutazione della significatività dipende da entrambi i valori F e p, e il livello di significatività scelto è basato sulla specifica analisi e sulle convenzioni del campo di studio.

#### Ipotesi

Nell'analisi statistica in cui si calcola un valore F, ci sono due ipotesi principali: l'ipotesi nulla (H0) e l'ipotesi alternativa (H1).

Ipotesi Nulla (H0): L'ipotesi nulla afferma che non ci sono differenze significative tra i gruppi o i fattori considerati.
In altre parole, l'ipotesi nulla sostiene che i parametri del modello o i fattori non hanno un effetto significativo sul risultato o che le differenze osservate sono casuali.
Nel contesto dell'analisi con un valore F, l'H0 suggerirebbe che non ci sono differenze significative tra i gruppi o i fattori.

Ipotesi Alternativa (H1 o HA): L'ipotesi alternativa è il contrario dell'ipotesi nulla.
Sostiene che ci sono differenze significative tra i gruppi o i fattori considerati, e che le differenze osservate non sono casuali, ma sono dovute a un effetto significativo dei parametri del modello o dei fattori.
Nel contesto dell'analisi con un valore F, l'ipotesi alternativa suggerirebbe che ci sono differenze significative tra i gruppi o i fattori.

Quindi, quando si calcola un valore F e si esegue un test statistico, l'ipotesi nulla (H0) è che non ci siano differenze significative, mentre l'ipotesi alternativa (H1 o HA) è che ci siano differenze significative.
L'obiettivo del test F è determinare se i dati forniscono prove sufficienti per respingere l'ipotesi nulla a favore dell'ipotesi alternativa, indicando che ci sono differenze significative tra i gruppi o i fattori considerati.

Continuiamo l'esempio di prima:

```{r}
# Visualizziamo la tabella ANOVA
anova_summary <- summary(anova_result)
print(anova_summary)

# Estraiamo il valore F
f_statistic <- anova_summary[[1]][["F value"]][3]  # Usiamo [3] per estrarre il valore relativo all'interazione

# Estraiamo il valore p
p_value <- anova_summary[[1]][["Pr(>F)"]][3]  # Usiamo [3] per estrarre il valore relativo all'interazione

cat("Valore F:", f_statistic, "\n")
cat("Valore p:", p_value, "\n")

# Scegliamo un livello di significatività (alpha)
alpha <- 0.05

# Valutiamo se rifiutare l'ipotesi nulla
if (p_value < alpha) {
  cat("Rifiutiamo l'ipotesi nulla. Ci sono differenze significative tra i gruppi.\n")
} else {
  cat("Non rifiutiamo l'ipotesi nulla. Non ci sono differenze significative tra i gruppi.\n")
}
```

## Nested Models

Nei modelli statistici, un "nested model" (modello nidificato) si verifica quando un modello più complesso o generale può essere suddiviso o semplificato in un modello più semplice o specifico.
Il modello più semplice è considerato "nidificato" all'interno del modello più complesso, poiché contiene un sottoinsieme di parametri o vincoli del modello più generale.

Nel contesto della regressione, i modelli nidificati sono spesso utilizzati per testare l'aggiunta di variabili indipendenti al modello al fine di valutare se le variabili aggiuntive migliorano significativamente la capacità di previsione o spiegazione del modello.
I modelli nidificati sono anche utilizzati in contesti come l'analisi della varianza (ANOVA), l'analisi della devianza nei modelli lineari generalizzati (GLM) e altre procedure statistiche.

Ecco un esempio per chiarire il concetto di modelli nidificati:

Esempio di Modelli di Regressione Nidificati:

Supponiamo di voler creare un modello di regressione per prevedere il reddito di una persona basandoci su tre variabili indipendenti: età, istruzione, esperienza lavorativa e genere.
Il modello completo potrebbe essere:

1 - Modello Completo

$$ Reddito = \beta_0 + \beta_1 \cdot Età + \beta_2 \cdot Istruzione + \beta_3 \cdot Esperienza + \beta_4 \cdot Genere $$ Tuttavia, potremmo essere interessati a valutare se l'aggiunta della variabile "genere" migliora significativamente la capacità predittiva del modello.
In tal caso, il modello senza "genere" è nidificato all'interno del modello completo:

2 - Modello Nidificato

$$ Reddito = \beta_0 + \beta_1 \cdot Età + \beta_2 \cdot Istruzione + \beta_3 \cdot Esperienza $$

In questo esempio, il Modello 1 è il modello completo e il Modello 2 è il modello nidificato senza il parametro per "genere".
Per confrontare questi modelli, si può utilizzare il test F per determinare se l'aggiunta di "genere" contribuisce in modo significativo alla spiegazione della variazione nei redditi.
Se il test F mostra che il modello completo è significativamente migliore del modello nidificato, possiamo concludere che "genere" è un predittore significativo.

In sintesi, i modelli nidificati sono utilizzati per valutare l'aggiunta o la rimozione di variabili o parametri nei modelli statistici al fine di determinare se tali modifiche migliorano significativamente la bontà di adattamento o la capacità predittiva del modello.

```{r}
# Creiamo dati fittizi
set.seed(123)
n <- 100
eta <- rnorm(n, mean = 35, sd = 5)
istruzione <- rnorm(n, mean = 12, sd = 2)
esperienza <- rnorm(n, mean = 10, sd = 3)
genere <- sample(c("Maschio", "Femmina"), n, replace = TRUE)
reddito <- 20 + 2 * eta + 3 * istruzione + 5 * esperienza + ifelse(genere == "Maschio", 4, 0) + rnorm(n, mean = 0, sd = 5)

# Creiamo un dataframe con i dati
data <- data.frame(eta, istruzione, esperienza, genere, reddito)

# Modello completo
modello_completo <- lm(reddito ~ eta + istruzione + esperienza + genere, data = data)

# Modello nidificato senza "genere"
modello_nidificato <- lm(reddito ~ eta + istruzione + esperienza, data = data)

# Test F per confrontare i modelli
anova_result <- anova(modello_nidificato, modello_completo)

# Visualizziamo la tabella ANOVA
print(anova_result)

# Scegliamo un livello di significatività (alpha)
alpha <- 0.05

# Valutiamo se rifiutare l'ipotesi nulla
if (anova_result[2, "Pr(>F)"] < alpha) {
  cat("La rimozione di 'genere' non migliora significativamente il modello.\n")
} else {
  cat("La rimozione di 'genere' migliora significativamente il modello.\n")
}

```

## Variable Selection in R

La "variable selection" (selezione delle variabili) in R è un processo attraverso il quale si scelgono le variabili più rilevanti da includere in un modello statistico.
Questo processo è utile per semplificare i modelli, migliorare la capacità predittiva e la comprensione dei dati, ridurre l'overfitting e aumentare l'efficienza computazionale.

Un metodo comune per la selezione delle variabili in R coinvolge l'utilizzo dell'Information Criterion (Criterio d'Informazione) di Akaike (AIC) insieme alla funzione step().

AIC (Akaike's Information Criterion):

Il Criterio d'Informazione di Akaike (AIC) è una metrica che misura la qualità di un modello statistico.
L'obiettivo dell'AIC è trovare il miglior compromesso tra la bontà di adattamento del modello ai dati e la sua complessità.
L'AIC tiene conto della funzione di verosimiglianza del modello e penalizza i modelli con un numero maggiore di parametri.
L'AIC è definito come:

$$ AIC = -2logLikelihood + 2k $$

Dove:

-   "log-likelihood" è il logaritmo della funzione di verosimiglianza del modello.
-   "k" è il numero di parametri stimati nel modello. Un valore AIC più basso indica un modello migliore, in quanto indica un migliore adattamento ai dati con meno complessità.

Funzione step():

La funzione step() in R è utilizzata per effettuare la selezione delle variabili basata su criteri come l'AIC.
Consente di confrontare e selezionare i modelli in modo automatico aggiungendo o rimuovendo variabili dal modello, fino a trovare il modello con l'AIC più basso.
La sintassi di base della funzione step() è la seguente:

```{r}
#step(modello_iniziale, direction = "both", scope = list(lower = modello_minimo, upper = modello_massimo))
```

-   modello_iniziale è il modello di partenza che desideri semplificare o migliorare.
-   direction può essere "forward", "backward", o "both" e specifica se aggiungere, rimuovere o entrambi i tipi di variabili durante la selezione.
-   scope specifica l'intervallo dei modelli da considerare durante la selezione. Il - "modello_minimo" rappresenta il modello più semplice possibile (ad esempio, un modello con solo l'intercetta), mentre il "modello_massimo" rappresenta il modello più complesso (il modello completo con tutte le variabili).

```{r}
# Carica il dataset di esempio
data(mtcars)

# Crea un modello lineare iniziale
modello_iniziale <- lm(mpg ~ ., data = mtcars)

# Esegui la selezione delle variabili basata su AIC
modello_selezionato <- step(modello_iniziale, direction = "both")
```

In questo esempio, partiamo da un modello lineare completo che utilizza tutte le variabili di mtcars, e poi utilizziamo step() per eseguire la selezione delle variabili basata su AIC.
Alla fine, otteniamo il modello con l'AIC più basso, che dovrebbe essere una versione semplificata del modello iniziale con solo le variabili più rilevanti.

La "variable selection" utilizzando AIC e step() è un potente strumento per migliorare la qualità e l'interpretabilità dei modelli statistici, in particolare quando si hanno molti potenziali predittori.

## Categorical Predictors and Interactions

Nell'analisi statistica, i "categorical predictors" (predittori categorici) sono variabili che rappresentano categorie o gruppi distinti anziché valori numerici.
Queste variabili sono anche conosciute come variabili qualitative o fattori.
Ad esempio, il genere (maschio/femmina), il livello di istruzione (scuola elementare, scuola media, laurea), o il tipo di prodotto (A, B, C) sono esempi di predittori categorici.
Quando si utilizzano predittori categorici in un modello statistico, è importante considerare come gestire e interpretare questi dati.

Una considerazione fondamentale è come rappresentare le variabili categoriche nel modello.
Solitamente, vengono utilizzate delle variabili dummy (variabili indicatrici) per rappresentare le categorie.
Ad esempio, nel caso del genere (maschio/femmina), potrebbero essere create due variabili dummy, una per il maschio e una per la femmina.
Queste variabili dummy prendono il valore 1 o 0 a seconda dell'appartenenza alla categoria.
Questo approccio consente al modello di catturare l'effetto della categoria sulla variabile dipendente.

Oltre alla rappresentazione delle variabili categoriche, è importante considerare le interazioni tra i predittori.
Le interazioni si verificano quando l'effetto di una variabile categorica sul risultato dipende da un'altra variabile.
Ad esempio, l'effetto del livello di istruzione sul reddito potrebbe variare in base al genere.
In questo caso, c'è un'interazione tra il livello di istruzione e il genere.

Per esaminare le interazioni tra predittori categorici, è possibile utilizzare l'analisi della varianza (ANOVA) o i modelli lineari generalizzati (GLM).
Le interazioni possono fornire informazioni preziose sull'influenza combinata delle variabili categoriche sul risultato.

```{r}
# Creiamo dati fittizi
set.seed(123)
n <- 100
genere <- sample(c("Maschio", "Femmina"), n, replace = TRUE)
istruzione <- rep(c("Elementare", "Media", "Laurea"), length.out = n )
reddito <- 30 + ifelse(genere == "Maschio", 5, 0) + ifelse(istruzione == "Laurea", 10, 0) + rnorm(n, mean = 0, sd = 5)

# Creiamo un dataframe con i dati
data <- data.frame(genere, istruzione, reddito)

# Modello con interazione tra genere e istruzione
modello <- lm(reddito ~ genere * istruzione, data = data)

# Visualizziamo i risultati
summary(modello)
```

In questo esempio, stiamo creando dati fittizi con due predittori categorici: "genere" e "istruzione".
Il modello lineare include un'interazione tra questi due predittori.
L'uso di \* nell'equazione del modello indica l'interazione.
La funzione lm viene utilizzata per adattare il modello.
La tabella dei risultati summary(modello) mostra come i predittori categorici e l'interazione influenzano il reddito.

Possiamo concludere che il genere e il livello di istruzione hanno un effetto significativo sul reddito, mentre le interazioni tra genere e istruzione non sono significative in questo modello.
Il modello nel suo complesso è significativo e in grado di spiegare una parte significativa della variazione nel reddito.

#### Factors with More Than Two Levels

Quando si affrontano fattori con più di due livelli (categorie), è necessario considerare come gestire queste variabili nel modello.
In generale, un fattore con k livelli richiede la creazione di k-1 variabili dummy per evitare la "dummy variable trap".
La "dummy variable trap" si verifica quando le variabili dummy sono linearmente dipendenti e possono portare a problemi di multicollinearità.

Ad esempio, se abbiamo una variabile "colore" con tre livelli (rosso, verde, blu), dovremmo creare due variabili dummy per rappresentarla.
Una rappresenterà il rosso e l'altra il verde.
Se entrambe le variabili dummy sono uguali a 0, ciò significa che il colore è blu.
Questo evita la trap della variabile dummy.

```{r}
# Creiamo dati fittizi
set.seed(123)
n <- 100
colore <- rep(c("Rosso", "Verde", "Blu"), length.out = n )
voto <- rnorm(n, mean = 50, sd = 10)

# Creiamo un dataframe con i dati
data <- data.frame(colore, voto)

# Modello con un fattore con più di due livelli
modello <- lm(voto ~ colore, data = data)

# Visualizziamo i risultati
summary(modello)
```

I risultati indicano che il colore del prodotto (rosso o verde) non ha un impatto significativo sul voto.
L'intercetta, che rappresenta il colore "Blu," è significativa, ma il modello nel suo insieme non è molto efficace nel spiegare la variazione nei voti.

## Model Checking

Model Checking è una fase cruciale nell'analisi statistica, specialmente quando si adotta un modello di regressione.
Durante questa fase, si valuta se il modello soddisfa le principali assunzioni dei modelli lineari.
Le quattro assunzioni principali da verificare sono:

-   **L**inearity (Linearità): Questa assunzione afferma che la risposta (variabile dipendente) può essere scritta come una combinazione lineare delle variabili predittive (variabili indipendenti). In altre parole, il modello dovrebbe essere in grado di catturare il rapporto tra le variabili in modo lineare, con un certo grado di rumore residuo. La linearità può essere verificata attraverso grafici di dispersione o grafici residui.
-   **I**ndependence (Indipendenza): Questa assunzione richiede che gli errori (residui) del modello siano indipendenti l'uno dall'altro. Ciò significa che il valore di errore per un'osservazione non è influenzato dal valore di errore per un'altra osservazione. L'indipendenza può essere verificata osservando i grafici dei residui in sequenza temporale o spaziale, a seconda del contesto.
-   **N**ormality (Normalità): L'assunzione di normalità richiede che i residui del modello seguano una distribuzione normale. Questo è importante perché molte procedure statistiche si basano sull'ipotesi di normalità dei residui. La normalità può essere verificata tramite grafici quantile-quantile (QQ plot) o istogrammi dei residui.
-   **E**qual Variance (Varianza Uniforme): Questa assunzione, chiamata anche omoschedasticità, richiede che la varianza dei residui sia costante in tutti i livelli delle variabili predittive. In altre parole, non dovrebbe esserci alcun modello discernibile nella varianza dei residui. La varianza uniforme può essere verificata osservando i grafici dei residui rispetto ai valori predetti.

Per verificare queste assunzioni, i Residuals-based displays (grafici basati sui residui) sono spesso utilizzati.
Questi includono:

-   Scatterplot dei residui: Un grafico dei residui contro i valori previsti o le variabili predittive. Questo può rivelare se c'è una struttura non lineare nei residui.
-   Grafico di sequenza temporale dei residui: Utilizzato quando i dati sono raccolti nel tempo, questo grafico può rivelare dipendenze temporali nei residui.
-   QQ-plot (Quantile-Quantile plot): Questo grafico confronta i quantili dei residui con quelli di una distribuzione normale. Se i punti del grafico seguono una linea retta, i residui sono approssimativamente normali.
-   Istogramma dei residui: Un istogramma dei residui può dare un'idea della loro distribuzione e normalità.

Rispettare queste assunzioni è importante per garantire che le stime del modello siano affidabili e che le conclusioni siano valide.
Se una o più di queste assunzioni non sono soddisfatte, potrebbero essere necessarie correzioni al modello o ai dati stessi.

## Transformations

Le trasformazioni sono una tecnica utilizzata nella modellazione statistica per modificare le relazioni tra variabili al fine di soddisfare meglio le assunzioni del modello.
Le trasformazioni possono essere utili quando le relazioni tra le variabili non sono lineari o quando le assunzioni di omoschedasticità o normalità dei residui non sono soddisfatte.
Di seguito, affrontiamo i seguenti argomenti relativi alle trasformazioni:

-   Variance Stabilizing Transformations (Trasformazioni per Stabilizzare la Varianza): In alcuni casi, la varianza dei dati può variare in modo non costante con il cambiare del valore medio.
    Questo fenomeno è noto come eteroschedasticità.
    Le trasformazioni possono essere utilizzate per stabilizzare la varianza, rendendo la relazione tra il valore medio e la varianza più costante.
    Un esempio comune è la trasformazione di Box-Cox.

-   Box-Cox Transform: La trasformazione di Box-Cox è una tecnica utilizzata per stabilizzare la varianza e rendere i dati approssimativamente normali.
    È definita come:

$$ 
y(\lambda) = \begin{cases} \frac{(y^\lambda - 1)}{\lambda} & \text{se } \lambda \neq 0 \\
    \log(y) & \text{se } \lambda = 0
\end{cases}
$$

Dove y sono i dati originali e λ è il parametro di trasformazione.
È possibile calcolare il valore ottimale di λ che massimizza la normalità dei dati.

```{r}
# Genera dati casuali
set.seed(123)
data <- data.frame(y = rgamma(100, shape = 2, scale = 1))

# Applica la trasformazione di Box-Cox
library(MASS)
result <- boxcox(y ~ 1, data = data)
lambda <- result$x[which.max(result$y)]
transformed_data <- if (lambda == 0) log(data$y) else ((data$y^lambda - 1) / lambda)

# Visualizza il valore ottimale di lambda
cat("Valore ottimale di lambda:", lambda, "\n")
```

-   Polynomials (Polinomi): Le trasformazioni polinomiali consentono di modellare relazioni non lineari tra variabili. È possibile aggiungere termini polinomiali al modello di regressione per catturare curve o relazioni più complesse. Ad esempio, si possono utilizzare polinomi di secondo grado per modellare una relazione quadratica tra una variabile indipendente e la variabile dipendente. L'aggiunta di termini polinomiali può migliorare l'adattamento del modello ai dati, ma è importante evitare di aggiungere troppi termini polinomiali per evitare il sovradattamento.

```{r}
# Modello lineare con un termine polinomiale di secondo grado
model <- lm(y ~ x + I(x^2), data = data)
```

-   Transformations of Predictor Variables (Trasformazioni delle Variabili Predittive): Le trasformazioni delle variabili predittive sono utilizzate per adattare i dati in modo che soddisfino meglio le assunzioni del modello. Queste trasformazioni coinvolgono la modifica delle variabili indipendenti piuttosto che della variabile dipendente. Possono essere utilizzate per rendere le relazioni tra le variabili più lineari o per stabilizzare la varianza. Ad esempio, è possibile applicare una trasformazione logaritmica o una radice quadrata a una variabile predittiva per renderla più lineare nei confronti della variabile dipendente.

```{r}
# Creiamo dati fittizi
set.seed(123)
X <- rnorm(100, mean = 10, sd = 2)
Y <- 2 * X + rnorm(100, mean = 0, sd = 1)

# Creiamo un dataframe con i dati
data <- data.frame(X, Y)

# Modello lineare senza trasformazione
model_no_transform <- lm(Y ~ X, data = data)

# Visualizziamo il summary del modello senza trasformazione
summary(model_no_transform)

# Trasformiamo la variabile X applicando il logaritmo
data$X_transformed <- log(data$X)

# Modello lineare con la variabile X trasformata
model_with_transform <- lm(Y ~ X_transformed, data = data)

# Visualizziamo il summary del modello con la variabile X trasformata
summary(model_with_transform)
```

-   Variance Stabilizing Transformations (Trasformazioni per Stabilizzare la Varianza): Queste trasformazioni sono utilizzate per stabilizzare la varianza dei residui nei modelli di regressione. In alcuni casi, la varianza dei residui può variare in modo non costante con il cambiare del valore medio delle variabili indipendenti. Questo fenomeno è noto come eteroschedasticità. Le trasformazioni, come la radice quadrata o la logaritmica, possono essere applicate alla variabile dipendente o alle variabili predittive per rendere la varianza dei residui più costante.

Le trasformazioni sono strumenti potenti per adattare i modelli ai dati in modo più accurato quando le relazioni tra variabili non sono lineari o quando le assunzioni del modello non sono soddisfatte.
Tuttavia, è importante scegliere con saggezza le trasformazioni per evitare il sovradattamento e garantire che i risultati siano interpretabili.

## Multicollinearity

La multicollinearità si verifica quando due o più variabili indipendenti in un modello di regressione sono fortemente correlate tra loro.
Questa correlazione tra le variabili indipendenti può rendere difficile l'interpretazione del modello e portare a stime poco affidabili dei coefficienti di regressione.
La presenza di multicollinearità può causare un aumento dei variabili inflation factor (VIF) è una misura comune utilizzata per valutare la multicollinearità tra le variabili indipendenti in un modello di regressione.
Un alto VIF per una variabile indica che quella variabile è fortemente correlata con le altre variabili indipendenti nel modello.

Variance Inflation Factor (VIF): Il VIF è una misura utilizzata per quantificare la multicollinearità tra le variabili indipendenti in un modello di regressione.
Un alto VIF indica che una variabile è altamente correlata con altre variabili indipendenti, il che può rendere difficile l'interpretazione del modello e portare a stime poco affidabili dei coefficienti di regressione.
Il VIF di ciascuna variabile indipendente è calcolato come il rapporto della varianza dell'errore standard del coefficiente di regressione stimato per quella variabile rispetto alla varianza dell'errore standard se la variabile fosse stata completamente non correlata alle altre variabili indipendenti.
In generale, un VIF superiore a 5 o 10 è spesso considerato un segno di multicollinearità significativa.

```{r}
# Load the necessary library
library(car)

# Create a sample dataset with multiple predictor variables
set.seed(123)
data <- data.frame(
  X1 = rnorm(100),
  X2 = rnorm(100),
  X3 = rnorm(100),
  X4 = rnorm(100)
)

# Add a dependent variable (response)
data$Y <- 2 * data$X1 + 3 * data$X2 + 1.5 * data$X3 + rnorm(100)

# Fit a linear regression model
model <- lm(Y ~ X1 + X2 + X3 + X4, data = data)

# Calculate VIF
vif_values <- vif(model)

# Print the VIF values
vif_values
```

Tutti i valori VIF sono vicini a 1, il che suggerisce che non c'è una forte multicollinearità tra le variabili predittive X1, X2, X3 e X4.
Questo è un buon segno, poiché significa che le variabili non sono fortemente correlate tra loro, il che rende più affidabili le stime dei coefficienti di regressione nel modello.

I valori VIF ci aiuteranno a valutare l'entità della collinearità nel modello.
Valori VIF più elevati indicano una multicollinearità più forte, e valori al di sopra di una certa soglia (ad esempio, VIF \> 5) possono suggerire la necessità di affrontare la collinearità, ad esempio, rimuovendo una delle variabili predittive correlate.

## Influential points

I punti influenti si riferiscono a osservazioni nei dati che hanno un impatto significativo sui risultati di un'analisi statistica, come una regressione lineare.
Questi punti possono influenzare la stima dei parametri del modello, i residui, i valori p, l'R-squared e altre statistiche di rilevanza.
Ci sono diverse metriche utilizzate per identificare i punti influenti, tra cui Standardized Residuals, Studentized Residuals e Cook's Distance.

-   Standardized Residuals (Residui Standardizzati): Questi sono i residui divisi per la deviazione standard dei residui. Un residuo standardizzato è una misura di quanto un punto dati si discosti dalla linea di regressione in termini di deviazioni standard. I punti con residui standardizzati molto grandi (positivi o negativi) sono considerati influenti.
-   Studentized Residuals (Residui Studentizzati): Questi sono i residui divisi per una stima della deviazione standard dell'errore residuo. I residui studentizzati sono utilizzati per valutare quanto un punto dati sia influente, considerando l'effetto delle altre osservazioni nel dataset. I punti con residui studentizzati significativamente grandi in valore assoluto sono considerati influenti.
-   Cook's Distance (Distanza di Cook): Cook's Distance è una metrica che combina l'effetto di un punto sui parametri del modello e il suo effetto sui residui. I punti con Cook's Distance molto grandi sono considerati influenti. Cook's Distance è spesso utilizzato per identificare punti che, se rimossi, avrebbero un impatto significativo sui risultati del modello.

Nel contesto della regressione, i punti influenti possono derivare da outlier nei dati, dati errati o punti che influenzano notevolmente la stima dei parametri.
Identificare e trattare i punti influenti è importante per garantire che il modello di regressione sia affidabile e rappresenti accuratamente i dati.
La rimozione di punti influenti può migliorare la bontà di adattamento del modello e l'accuratezza delle previsioni.

```{r}
# Carichiamo il dataset di esempio
data(mtcars)

# Adattiamo un modello di regressione lineare
model <- lm(mpg ~ wt + hp, data = mtcars)

# Calcoliamo i residui standardizzati
standardized_residuals <- rstandard(model)

# Identifichiamo i punti influenti basati sui residui standardizzati
influential_points <- which(abs(standardized_residuals) > 2)

# Visualizziamo gli indici dei punti influenti
cat("Punti influenti basati sui residui standardizzati:", influential_points, "\n")

# Calcoliamo i residui studentizzati
studentized_residuals <- rstudent(model)

# Identifichiamo i punti influenti basati sui residui studentizzati
influential_points_studentized <- which(abs(studentized_residuals) > 2)

# Visualizziamo gli indici dei punti influenti basati sui residui studentizzati
cat("Punti influenti basati sui residui studentizzati:", influential_points_studentized, "\n")

# Calcoliamo Cook's Distance
cook_distance <- cooks.distance(model)

# Identifichiamo i punti influenti basati su Cook's Distance
influential_points_cook <- which(cook_distance > 4 / length(cook_distance))

# Visualizziamo gli indici dei punti influenti basati su Cook's Distance
cat("Punti influenti basati su Cook's Distance:", influential_points_cook, "\n")
```
